from bs4 import BeautifulSoup
from multiprocessing import Pool
import lxml
from tkinter import *
import urllib
import tweepy
import ssl
import re
from pytrends.request import TrendReq

def main():

    GUI()
    WebCrawler(keyword1)
    WebCrawler(keyword2)
''' Keyword_Value_list = WebCrawler(Keyword_List,Algorthim_List,Website_List)
    PLotter(Keyword_Value_List,Plot_Type)'''

class GUI():
    def __init__(self) :
        def done():
            #easier for use later on
            global keyword1 ,keyword2 ,Plot_Type,algorithm_list
            keyword1 = textbox1.get()
            keyword2 = textbox2.get()
            Plot_Type = D_variable.get()
            algorithm_list = C_var.get()
            master.destroy()
            
        def open_Website_list():
            try:
                print("These are current Websites in list")
                with open('Website_list.txt','r') as f:
                    for line in f:
                        for word in line.split():
                            print(word)
    
            except IOError:
                f= open("Website_list.txt","w+")
                f.close
                with open('Website_list.txt','r') as f:
                    for line in f:
                        for word in line.split():
                            print(word)
    
            
        def Update_Website_list():
            f = open("Website_list.txt","a")
            for i in range(10):
                website = input("add a site ")
                website = " "+website
                f.write(website)
                print("done")
                f.close
        
        def Reset_Website_list():
            f= open("Website_list.txt","w+")
            f.close 
        
        def Open_Algorithm_list():
            try:
                print("The currernt list is")
                with open('Algorithm_list.txt','r') as f:
                    for line in f:
                        for word in line.split():
                            print(word)
            except IOError:
                f= open("Algorithm_list.txt","w+")
                f.close
                with open('Algorithm_list.txt','r') as f:
                    for line in f:
                        for word in line.split():
                            print(word)
            
            
        def Update_Algorithm_list():
                f = open("Algorithm_list.txt","a")
                try:
                    with open('Website_list.txt','r') as t:
                        for line in t:
                            for word in line.split():
                                website = input(word +" ")
                                website = (word + "," + website + " " )
                                f.write(website)
                                print("done")
                except IOError:
                    print("need to make website list first")
        def Reset_Algorithm_list():
            f= open("Algorithm_list.txt","w+")
            f.close 
        
        master = Tk()
    
        #the labels for the textboxs 
        Label(master, text="Search Term 1").grid(row=0)
        Label(master, text="Search Term 2").grid(row=1)
    
        #The Textbox code
        textbox1 = Entry(master)
        textbox1.grid(row=0, column=1)
        textbox2 = Entry(master)
        textbox2.grid(row=1, column=1)
        # an exitbutton
        Button(master, text="Scraper", command=done).grid(row=5, column=2, sticky=W, pady=4)
        Button(master, text="Graph", command=done).grid(row=5, column=3, sticky=W, pady=4)
        #the dropbox code
        D_variable = StringVar(master)
        D_variable.set("Bar Chart") # default value
        dropbox = OptionMenu(master, D_variable, "Bar Chart", "Line Graph").grid(row=2, column=1)
    
        #the checkbox code
        C_var = IntVar()
        c = Checkbutton(master, text="Algorithm", variable=C_var).grid(row=3, column=0)
    
        #the website list buttons
        Button(master, text="website list", command=open_Website_list).grid(row=4,column=1)
        Button(master, text="Update", command=Update_Website_list).grid(row=4,column=2) 
        Button(master, text="Reset", command=Reset_Website_list).grid(row=4,column=3)
    
        #the algorithm buttons
        Button(master, text="Algorithm list", command=Open_Algorithm_list).grid(row=3,column=1)
        Button(master, text="Update", command=Update_Algorithm_list).grid(row=3,column=2) 
        Button(master, text="Reset", command=Reset_Algorithm_list).grid(row=3,column=3)
    
        mainloop()

#the webcrawler
def WebCrawler(keyword):
    global keyW
    keyW = keyword
    
    Final_tally = 0
    print(algorithm_list)
    with open('Algorithm_list.txt','r') as f:
        for line in f:
            for word in line.split():
                tally = 0
                Site_tally = 0
                print(word[-1] +" - "+ word[0:-2])
                url = word[0:-2]          
                #Seperates the website and the algorithm
                if "twitter" in url:
                    tally = Twittercrawler()
                elif "google" in url:
                    tally = GoogleCrawler()
                else:
                    tally = NormalLink(url)
                    
                if algorithm_list == 1:                    
                    print(int(word[-1]))
                    Site_tally = tally * int(word[-1])
                else:
                    Site_tally = tally        
                print("site tally =", Site_tally)
                Final_tally = Final_tally + Site_tally
    print("final tally =", Final_tally)


def SaveThefile(Count):
    try:

        with open('Algorithm_list.txt','r') as f:
            for line in f:
                for word in line.split():
                    print(word)
    except IOError:
        f= open("Algorithm_list.txt","w+")
        f.close
        with open('Algorithm_list.txt','r') as f:
            for line in f:
                for word in line.split():
                    print(word)  
                    
def Twittercrawler():
    result = 0
    consumer_key = 'vpq1bJAwOSyHUvLkJslbOQOl4'
    consumer_secret = 'xJC2ndsF0uZLYJmOZPEC77sI5oRe0trJTMBrkuPkhFwLXzRQTF'
    access_token = '4527193223-vUzk3OhAtMLANfjScS39IW7oZM7MTly0J1T5Rxv'
    access_token_secret = 'NDDqu9vSXvcOD6xZEAjgGd9oBSuPlWSunU88zpCaWOsRz'

    auth = tweepy.OAuthHandler(consumer_key, consumer_secret)
    auth.set_access_token(access_token, access_token_secret)

    api = tweepy.API(auth)
    try:
        for tweet in tweepy.Cursor(api.search,q=keyW,rpp=100,count=20,result_type="recent",include_entities=True,lang="en").items(5000):
            result = result + 1
    except tweepy.error.TweepError:
        print("you exceeded tweet limit")
    return result

def GoogleCrawler():
    result = 0


    # Login to Google. Only need to run this once, the rest of requests will use the same session.
    pytrend = TrendReq("projectcodea2@gmail.com", "proj3ctcod3A2")
    
    # Create payload and capture API tokens needed for related_queries()
    pytrend.build_payload(kw_list=[keyW])
    
    # Related Queries, returns a dictionary of dataframes
    related_queries_dict = pytrend.related_queries()
    # Spliting up the Queries to only get numbers
    Search = related_queries_dict[keyW]
    Rising = Search['rising']
    Value  = Rising['value']
    for x in Value:
        result = result + int(x)
        
    return result

def NormalLink(url):
    context = ssl._create_unverified_context()#needed to get round school sercurity settings
    content = urllib.request.urlopen(url, context=context).read()
    soup = BeautifulSoup(content,lxml)
    #loads up the main page of the site to be scraped from
    linklist = []
    
    for link in soup.find_all('a'):           
        link = (link.get('href'))      
        #gets the children sites from the main one and goes though them
        try:
            if link[0:len(url)] == url:                           
                pass
            elif link[0:4] == 'http':
                pass
            else:
                link = (url + link)                
        except TypeError:
            pass
                    
                    # sometimes the href returned would just be /food/ this adds ending

        linkChecker(link,linklist)
                    #check to make sure link hasn't been used before   
    print("linklist = " , len(linklist))                                     
                         
    with Pool(8) as p:
        result = (p.map(WebScraper,linklist))
                        
    tally = 0
    for x in result:
        tally = x + tally
    return tally     
                
#exists to make sure link has not allready been scraped
def linkChecker(link,linklist):  
    if link == None:
        return False
    if link[0] == "#":
        print("#")
        return False
        
    #checks for a # at the start of a site as if it has one system dosn't work
    else:
        if len(linklist) == 0:
            linklist.insert(0,link)
            pass
            #done to avoid the empty list problem
        else:
            
            for x in linklist: 
                if x == link:
                    return False
                    #checks to make sure a link doesn't repeat 
                else:
                    linklist.insert(0,link)
                    return True 
                

#the webscraper itselfs
def WebScraper(link):
    context = ssl._create_unverified_context()#needed to get round school sercurity settings
    try:
        content = urllib.request.urlopen(link, context=context).read()
        soup = BeautifulSoup(content)
        results = soup.body.find_all(string=re.compile('.*{0}.*'.format(keyW)), recursive=True)
        results = len(results)
    except AttributeError:
        results = 0
    except ValueError:
        results = 0
    except urllib.error.HTTPError:
        results = 0
    except urllib.error.URLError:
        results = 0
    return results

main()
